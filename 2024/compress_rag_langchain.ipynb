{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Compress RAG Contexts In LangChain\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is a powerful framework for providing LLMs the necessary context for a specific use case. Unfortunately, token usage can get quite expensive, especially if your RAG setup is returning dozens of larger files. Also research has indicated that LLMs may struggle to find the correct result if the result is buried in the middle of your RAG results.\n",
    "\n",
    "Luckily, there are context compression solutions to solve this.\n",
    "\n",
    "### Context Compression in LangChain\n",
    "\n",
    "In this tutorial, we will set up contextual compression in LangChain using [ContextCrunch](https://contextcrunch.com) for efficient compression.\n",
    "\n",
    "**Aside: LangChain**\n",
    "\n",
    "LangChain is a powerful tool to build LLM-centric data pipelines in an intuitive manner. This tutorial will assume you already are somewhat familiar with RAG and LangChain. If you would like to learn how to build a simple LangChain RAG pipeline, check out [their documentation](LINK NEEDED).\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "You need to install the ContextCrunch-LangChain integration in addition to langchain, as well as the OpenAI python SDK. You can install everything with `pip install contextcrunch-langchain openai`  \n",
    "\n",
    "### Get Started\n",
    "\n",
    "We’ll first initialize the OpenAI client, as well as create a mock retriever to act in place of whatever document retriever you are using. This way, we can obtain a consistent output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matt/Documents/GitHub/Blogs/env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import requests\n",
    "\n",
    "gpt4_chat = ChatOpenAI(model_name=\"gpt-4-1106-preview\", max_tokens=50, temperature=0, api_key=\"YOUR_OPENAI_API_KEY_HERE\")\n",
    "\n",
    "class MockBaseRetriever(BaseRetriever):\n",
    "    documents: List[Document] = []\n",
    "\n",
    "    def __init__(self, documents):\n",
    "        super().__init__()\n",
    "        self.documents = documents\n",
    "\n",
    "    def get_relevant_documents(self, *args, **kwargs):\n",
    "        return self.documents\n",
    "\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/Speuce/Blogs/master/2024/rag_text.txt'\n",
    "\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "documents = [Document(page_content=content) for content in text.split('\\n\\n')]\n",
    "retriever = MockBaseRetriever(documents=documents)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll retrieve a text sample emulating a large RAG QA result, from the lost-in-the-middle dataset, and convert it into separate LangChain `Documents`. Finally we’ll wrap the documents in our `MockBaseRetriever` from before.\n",
    "\n",
    "- Aside: Documents are TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Write a high-quality answer for the given question using only the provided search results.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "question = \"in the dynastic cycle what is the right to rule called\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template, and Question\n",
    "\n",
    "Let’s define a prompt template to use, as well as the corresponding question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "        prompt_template\n",
    "        | gpt4_chat \n",
    "        | StrOutputParser()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: In the context of the dynastic cycle, particularly as it pertains to Chinese history and philosophy, the right to rule is called the \"Mandate of Heaven\" (Tianming). This concept held that the Emperor was chosen by Heaven—the,\n",
      " callback: Tokens Used: 3912\n",
      "\tPrompt Tokens: 3862\n",
      "\tCompletion Tokens: 50\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.04012\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    result = rag_chain.invoke({\"question\": question, \"context\": text})\n",
    "    print(f'Result: {result},\\n callback: {cb}')\n",
    "    original_prompt_cost = cb.total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is right! “Mandate of heaven” is what we’re looking for here. Unfortunately, nearly 4k tokens were used in this context. Let’s look at reducing it.\n",
    "\n",
    "### Context Compression\n",
    "\n",
    "Now that we have a set of documents retrieved, we can get to the meat of prompt compression.\n",
    "\n",
    "First, we import and instantiate an instance of  `ContextCrunchDocumentCompressor`. Ensure that you have your ContextCrunch api key, which you can get at https://contextcrunch.com/console/keys  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextcrunch_langchain import ContextCrunchDocumentCompressor\n",
    "\n",
    "cc_compressor = ContextCrunchDocumentCompressor(compression_ratio=0.9, api_key=\"YOUR_CONTEXT_CRUNCH_API_KEY_HERE\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will wrap the document compressor around the retriever from earlier using LangChain's `ContextualCompressionRetriever`. If you're using any other retriever (such as that from a vector DB), you would similarly wrap that retriever in the `ContextualCompressionRetriever` as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "contextcrunch_compression_retriever = ContextualCompressionRetriever(base_compressor=cc_compressor, base_retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a modified pipeline that uses the compression retriever we created. In order to format the output documents as a single text block to feed into the prompt, we also declare a `format_docs` function to join the documents with newlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain_2 = (\n",
    "    {\"context\": contextcrunch_compression_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | gpt4_chat \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the chain (this time only invoking it with the question, since the context comes from the retriever), and compare token usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: In the dynastic cycle, the right to rule is called the \"Mandate of Heaven.\" This concept originated in ancient China and was used to justify the rule of the Emperor. According to this belief, Heaven, which was a supreme force of,\n",
      " callback: Tokens Used: 438\n",
      "\tPrompt Tokens: 388\n",
      "\tCompletion Tokens: 50\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00538\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    result = rag_chain_2.invoke(question)\n",
    "    print(f'Result: {result},\\n callback: {cb}')\n",
    "    new_tokens = cb.total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the correct anwser of \"Mandate of Heaven\" is still present. The prompt compression worked!\n",
    "\n",
    "### Cost Savings\n",
    "Now we can calculate how much we saved as a percentage of the original cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost savings: 86.5902293120638%\n"
     ]
    }
   ],
   "source": [
    "cost_savings = (original_prompt_cost - new_tokens) / original_prompt_cost\n",
    "print(f\"Cost savings: {cost_savings*100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
